---
title: 上下文缓存
update: 2025-11-18
source: https://help.aliyun.com/zh/model-studio/context-cache
---

上下文缓存（Context Cache）通过缓存多个请求的公共前缀以减少重复计算，提升响应速度并降低成本。提供隐式缓存与显式缓存两种模式。

## 隐式缓存

- 自动模式，无需配置；命中概率不确定
- 命中部分按输入 Token 单价的 20% 计费

### 提升命中概率

- 将重复内容置于提示词前部；差异内容置于后部
- 多模态：同图多问时将图像放在文本前；不同图同问时将文本放在图像前

### 计费与用量统计

- 命中缓存的输入 Token 计费为 `cached_tokens`；未命中部分为标准 `input_tokens`
- 通过 `usage.prompt_tokens_details.cached_tokens` 查看命中数量（不同地域与模型字段略有差异）

## 显式缓存

- 需显式创建缓存块，有效期 5 分钟（命中后重置）
- 创建缓存时 Token 计费按输入单价的 125%；后续命中仅 10%
- 最小缓存长度：1024 Token；每次最多 4 个缓存标记

### 使用方式（OpenAI 兼容）

```python
from openai import OpenAI
import os

client = OpenAI(
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

long_text_content = "<Your Code Here>" * 400  # 约 1024+ Token

def get_completion(user_input):
    messages = [
        {
            "role": "system",
            "content": [{
                "type": "text",
                "text": long_text_content,
                "cache_control": {"type": "ephemeral"}
            }],
        },
        {"role": "user", "content": user_input},
    ]
    return client.chat.completions.create(
        model="qwen3-coder-plus",
        messages=messages,
    )

first = get_completion("这段代码的内容是什么")
print(first.usage.prompt_tokens_details.cache_creation_input_tokens)
print(first.usage.prompt_tokens_details.cached_tokens)
second = get_completion("这段代码可以怎么优化")
print(second.usage.prompt_tokens_details.cache_creation_input_tokens)
print(second.usage.prompt_tokens_details.cached_tokens)
```

### 使用方式（DashScope Python）

```python
import os
from dashscope import Generation

long_text_content = "<Your Code Here>" * 400

def get_completion(user_input):
    messages = [
        {
            "role": "system",
            "content": [{
                "type": "text",
                "text": long_text_content,
                "cache_control": {"type": "ephemeral"},
            }],
        },
        {"role": "user", "content": user_input},
    ]
    return Generation.call(
        api_key=os.getenv("DASHSCOPE_API_KEY"),
        model="qwen3-coder-plus",
        messages=messages,
        result_format="message",
    )

first = get_completion("这段代码的内容是什么")
print(first.usage.prompt_tokens_details["cache_creation_input_tokens"]) 
print(first.usage.prompt_tokens_details["cached_tokens"]) 
second = get_completion("这段代码可以怎么优化")
print(second.usage.prompt_tokens_details["cache_creation_input_tokens"]) 
print(second.usage.prompt_tokens_details["cached_tokens"]) 
```

## 典型场景

- 基于长文本的问答：相同系统提示与长文前缀易命中缓存
- 代码自动补全：前缀代码稳定，缓存提升补全速度
- 多轮对话：前缀逐轮累积，命中带来显著成本与延迟优化
- 角色扮演与 Few-shot：稳定的系统提示易复用缓存
- 视频理解：同视频多问将 `video` 在前，提高命中概率
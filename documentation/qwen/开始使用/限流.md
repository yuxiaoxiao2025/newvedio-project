---
title: 限流
update: 2025-11-18
source: https://help.aliyun.com/zh/model-studio/rate-limit
---

阿里云百炼为保障公平与稳定，对各模型设定基础限流。限流与主账号关联，按主账号下所有 API-KEY 的总调用量计算。相关模型能力与价格参见[模型列表](模型列表.md)与[模型价格](模型价格.md)。

## 限流规则

- 主账号维度：主账号下所有 RAM 子账号、业务空间与 API-KEY 的调用总和计入限流。
- 模型独立限流：不同模型具有各自的 RPM（每分钟请求数）与 TPM（每分钟 Token 消耗）限制。
- 秒级限制：服务可能按 RPS（RPM/60）与 TPS（TPM/60）限制，瞬时高峰也可能触发限流。

## 常见报错与含义

- Requests rate limit exceeded / You exceeded your current requests list：调用频率触发限流。
- Allocated quota exceeded / You exceeded your current quota：Token 消耗触发限流。
- Request rate increased too quickly：短时间频率骤增触发稳定性保护。

## 如何避免限流

- 选用高限流模型：优先使用 `qwen-plus` 等限流更宽松的稳定版或最新版。
- 优化调用策略：平滑请求速率（匀速调度、指数退避、请求队列缓冲），减小瞬时峰值；降低频率或缩短输入输出以减少 Token 消耗。
- 任务拆分与 Batch：将大任务拆分为小批次；可使用批量推理（Batch API），不受实时限流约束。
- 添加备选模型：限流时自动切换到备用模型继续生成。

### Python 异常处理与备选模型示例

```python
import os
import asyncio
from openai import AsyncOpenAI, APIStatusError

API_KEY = os.getenv("DASHSCOPE_API_KEY")
PRIMARY = "qwen-plus-2025-07-28"
FALLBACK = "qwen-plus-2025-07-14"

client = AsyncOpenAI(api_key=API_KEY, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")

async def send(model: str) -> bool:
    try:
        await client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": "你是谁？"}],
        )
        return True
    except APIStatusError as e:
        if e.status_code == 429:
            return False
        raise

async def main():
    ok = await send(PRIMARY)
    if not ok:
        ok = await send(FALLBACK)
    print("success" if ok else "failed")

asyncio.run(main())
```

## 参考限流值（示例）

| 模型 | 每分钟调用次数（RPM） | 每分钟消耗Token数（TPM） |
| --- | --- | --- |
| qwen3-max（北京） | 600 | 1,000,000 |
| qwen3-max-2025-09-23（北京） | 60 | 100,000 |
| qwen-plus（北京） | 15,000 | 5,000,000 |
| qwen-flash（北京） | 15,000 | 10,000,000 |
| qwen-turbo（北京） | 1,200 | 5,000,000 |

> 注：不同地域与版本的限流值不同，完整表格请以官方文档为准。